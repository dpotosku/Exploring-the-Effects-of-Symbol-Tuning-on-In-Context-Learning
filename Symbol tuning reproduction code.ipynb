{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To choose which GPU to use\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "#os.environ['TF_CUDNN_RESET_RND_GEN_STATE'] = '1'\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability and GPU details in PyTorch\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # For PyTorch\n",
    "print(torch.cuda.device_count())  # Number of GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your access token. For more information you can go: https://huggingface.co/docs/hub/en/security-tokens\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login using your access token\n",
    "login(token= \"your access token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload datasets from HuggingFace\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "import pandas as pd\n",
    "\n",
    "datasets = {\n",
    "    'rte': load_dataset(\"super_glue\", \"rte\", trust_remote_code=True),\n",
    "    'wnli': load_dataset(\"nyu-mll/glue\", \"wnli\"),\n",
    "    'qnli': load_dataset(\"nyu-mll/glue\", \"qnli\"),\n",
    "    'mnli': load_dataset(\"nyu-mll/glue\", \"mnli\"),\n",
    "    'snli': load_dataset(\"stanfordnlp/snli\"),\n",
    "    'cb': load_dataset(\"super_glue\", \"cb\", trust_remote_code=True),\n",
    "    'sst2': load_dataset(\"nyu-mll/glue\", \"sst2\"),\n",
    "    'rt': load_dataset(\"cornell-movie-review-data/rotten_tomatoes\"),\n",
    "    'qqp': load_dataset(\"nyu-mll/glue\", \"qqp\"),\n",
    "    'mrpc': load_dataset(\"nyu-mll/glue\", \"mrpc\"),\n",
    "    'pawsx': load_dataset(\"google-research-datasets/paws-x\", \"en\"),\n",
    "    'copa': load_dataset(\"super_glue\", \"copa\", trust_remote_code=True),\n",
    "    'piqa': load_dataset(\"ybisk/piqa\", trust_remote_code=True),\n",
    "    'agn': load_dataset(\"fancyzhx/ag_news\"),\n",
    "    'trec': load_dataset(\"SetFit/TREC-QC\"),\n",
    "    'wsc': load_dataset(\"super_glue\", \"wsc\", trust_remote_code=True),\n",
    "    'teo': load_dataset(\"christophsonntag/OLID\"),\n",
    "    'tei': load_dataset(\"Parth1612/pp_distilbert_ft_tweet_irony\"),\n",
    "    'wic': load_dataset(\"super_glue\", \"wic\", trust_remote_code=True),\n",
    "    'cola': load_dataset(\"nyu-mll/glue\", \"cola\"),\n",
    "    'wino': pd.read_json(\"/work/dpotosku/WINO dataset/train_xl.jsonl\", lines=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the correct split for each dataset. Since each uploaded dataset contains: train, test, and, possibly, validation splits, it's necessary to take the one we need.\n",
    "\n",
    "def unpacking_datsets(dataset_name, dataset, split):\n",
    "\n",
    "    \"\"\"  \n",
    "    Unpacks the dataset based on its name and split type.  \n",
    "      \n",
    "    - For 'wino', returns the dataset as-is.  \n",
    "    - For 'mnli' (validation), selects 'validation_matched'.  \n",
    "    - For several datasets ('snli', 'rt', 'pawsx', 'agn', 'trec', 'teo', 'tei') with 'validation' split,  \n",
    "      selects the 'test' subset instead.  \n",
    "    - Otherwise, returns the dataset for the given split.  \n",
    "    \"\"\" \n",
    "    \n",
    "    if dataset_name == 'wino':\n",
    "        unpacked_datset = dataset\n",
    "    elif dataset_name == 'mnli' and split == 'validation':\n",
    "        unpacked_datset = dataset['validation_matched']\n",
    "    elif dataset_name == 'snli' and split == 'validation':\n",
    "        unpacked_datset = dataset['test']\n",
    "    elif dataset_name == 'rt' and split == 'validation':\n",
    "        unpacked_datset = dataset['test']\n",
    "    elif dataset_name == 'pawsx' and split == 'validation':\n",
    "        unpacked_datset = dataset['test']\n",
    "    elif dataset_name == 'agn' and split == 'validation':\n",
    "        unpacked_datset = dataset['test']\n",
    "    elif dataset_name == 'trec' and split == 'validation':\n",
    "        unpacked_datset = dataset['test']\n",
    "    elif dataset_name == 'teo' and split == 'validation':\n",
    "        unpacked_datset = dataset['test']\n",
    "    elif dataset_name == 'tei' and split == 'validation':\n",
    "        unpacked_datset = dataset['test']\n",
    "    else:\n",
    "        unpacked_datset = dataset[split]\n",
    "        \n",
    "    return unpacked_datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 'train'\n",
    "\n",
    "train_split_datasets = {\n",
    "    name: unpacking_datsets(name, dataset, train_split)\n",
    "    for name, dataset in datasets.items()  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the block.\n",
    "\n",
    "#test_split = 'validation'\n",
    "\n",
    "#test_split_dataset = {\n",
    "#    name: unpacking_datsets(name, dataset, test_split)\n",
    "#    for name, dataset in datasets.items()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(dataset_name, dataset):\n",
    "    \"\"\"\n",
    "    Preprocess a training dataset by removing unnecessary columns.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_name (str): The name of the dataset.\n",
    "                            If 'wino', the dataset is already a Pandas DataFrame.\n",
    "                            Otherwise, the 'train' split of the dataset will be converted to a Pandas DataFrame.\n",
    "        dataset (Dataset or pd.DataFrame): The input dataset. This can be either:\n",
    "                                           - A Pandas DataFrame (if `dataset_name` is 'wino').\n",
    "                                           - A Hugging Face Dataset object containing a 'train' split.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with specified columns removed.\n",
    "    \"\"\"\n",
    "    # Determine if the dataset is already in DataFrame format or needs conversion.\n",
    "    if dataset_name == 'wino':\n",
    "        # Dataset is already a Pandas DataFrame for 'wino'.\n",
    "        train_dataset = dataset\n",
    "    else:\n",
    "        # Convert the 'train' split of the dataset to a Pandas DataFrame.\n",
    "        train_dataset = dataset.to_pandas()\n",
    "\n",
    "    # List of columns to drop from the dataset.\n",
    "    columns_to_drop = [\n",
    "        'idx', 'processed_input', 'id', 'label_nw', 'label_original',\n",
    "        'label_coarse', 'label_coarse_original', 'span1_index', 'span2_index',\n",
    "        'input_ids', 'attention_mask', 'cleaned_tweet', 'subtask_b', 'subtask_c',\n",
    "        'phrase1', 'qID', 'start1', 'start2', 'end1', 'end2', 'ID', 'Organization name',\n",
    "        'Target'\n",
    "    ]\n",
    "\n",
    "    # Drop the specified columns. Ignore errors if columns are missing.\n",
    "    new_dataset = train_dataset.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing annesessary columns from the datasets and creating a dictionary with the modified datasets\n",
    "\n",
    "train_without_columns = {\n",
    "    name: remove_columns(name, dataset)\n",
    "    for name, dataset in train_split_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the block.\n",
    "\n",
    "#test_without_columns = {\n",
    "#    name: remove_columns(name, dataset)\n",
    "#    for name, dataset in test_split_dataset.items()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the datasets to have a unified structure: 'text', 'dataset_name', and 'label',\n",
    "# where 'text' contains all the values related to 'label. For example, the WIC dataset has the main columns:\n",
    "# 'sentence 1', 'sentence 2', 'word', and 'label'.\n",
    "# The 'text' column will contain a concatenation of the columns' values 'sentence 1', 'sentence 2', and 'word'.\n",
    "# As their combination corresponds to the value in the 'label' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREC dataset contains 'label' and 'label_text' columns that are annecessary, and\n",
    "# the 'label_coarse_text' column representing text-labels. The 'label' and 'label_text' columns\n",
    "# will be dropped, and the 'label_coarse_text' column's name will be changed to 'label'.\n",
    "\n",
    "drop_columns = ['label', 'label_text']\n",
    "\n",
    "cleaned_trec_train = train_without_columns['trec'].drop(columns = drop_columns)\n",
    "cleaned_trec_train = cleaned_trec_train.rename(columns = {'label_coarse_text': 'label'})\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#cleaned_trec_test = test_without_columns['trec'].drop(columns = drop_columns)\n",
    "#cleaned_trec_test = cleaned_trec_test.rename(columns = {'label_coarse_text': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_without_columns['trec'] = cleaned_trec_train\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#test_without_columns['trec'] = cleaned_trec_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEO dataset contains the 'subtask_a' column that represents label-column, but it has incorrect name.\n",
    "# The 'subtask_a' column will be renamed to 'label'.\n",
    "\n",
    "cleaned_teo_train = train_without_columns['teo'].rename(columns = {'subtask_a':'label'})\n",
    "\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#cleaned_teo_test = test_without_columns['teo'].rename(columns = {'subtask_a':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_without_columns['teo'] = cleaned_teo_train\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#test_without_columns['teo'] = cleaned_teo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIC dataset has incorrect columns order (accordin to the papaer's examples it's supposed to have the following order:\n",
    "# 'sentence1', 'sentence2', 'phrase2', 'label'). It will be changed.\n",
    "\n",
    "new_order = ['sentence1', 'sentence2', 'word', 'label']\n",
    "\n",
    "cleared_wic_train = train_without_columns['wic'].copy()\n",
    "cleared_wic_train = cleared_wic_train[new_order]\n",
    "\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#cleared_wic_test = test_without_columns['wic'].copy()\n",
    "#cleared_wic_test = cleared_wic_test[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_without_columns['wic'] = cleared_wic_train\n",
    "\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#test_without_columns['wic'] = cleared_wic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINO dataset has the 'answer' column that reprepents labels. Its name will be changed to 'label'.\n",
    "\n",
    "cleared_wino = train_without_columns['wino'].rename(columns = {'answer':'label'})\n",
    "\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#cleared_wino = test_without_columns['wino'].rename(columns = {'answer':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_without_columns['wino'] = cleared_wino\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#test_without_columns['wino'] = cleared_wino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def change_labels_in_datasets_with_optional_mappings(\n",
    "    mappings: Dict[str, dict], \n",
    "    datasets: Dict[str, pd.DataFrame]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Replace numeric values in the 'label' column for all datasets in a dictionary,\n",
    "    using a different mapping for each dataset. If no mapping is provided for a dataset,\n",
    "    its 'label' values remain unchanged.\n",
    "    \n",
    "    Args:\n",
    "        mappings (Dict[str, dict]): A dictionary where keys are dataset names and\n",
    "                                    values are dictionaries mapping numeric values to text labels.\n",
    "        datasets (Dict[str, pd.DataFrame]): A dictionary where keys are dataset names and\n",
    "                                            values are pandas DataFrames with a 'label' column.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: A new dictionary with updated datasets.\n",
    "    \"\"\"\n",
    "    updated_datasets = {}\n",
    "\n",
    "    for name, dataset in datasets.items():\n",
    "        # Validate that the dataset contains the 'label' column\n",
    "        if 'label' not in dataset.columns:\n",
    "            raise ValueError(f\"Dataset '{name}' does not contain a 'label' column.\")\n",
    "        \n",
    "        # Get the mapping for the current dataset, or None if no mapping is provided\n",
    "        mapping = mappings.get(name)\n",
    "        \n",
    "        # If no mapping is provided, keep the dataset unchanged\n",
    "        if mapping is None:\n",
    "            updated_datasets[name] = dataset.copy()\n",
    "        else:\n",
    "            # Apply the mapping\n",
    "            updated_dataset = dataset.copy()\n",
    "            updated_dataset['label'] = updated_dataset['label'].replace(mapping)\n",
    "            updated_datasets[name] = updated_dataset\n",
    "\n",
    "    return updated_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TREC, WSC, and SUBJ datasets, there is no need to change values in the 'label' columns, \n",
    "# because they already contain text labels.\n",
    "\n",
    "mappings = {\n",
    "    'rte': {0: 'entailment', 1: 'not entailment'},\n",
    "    'wnli': {0: 'not entailment', 1: 'entailment'},\n",
    "    'qnli': {0: 'entailment', 1: 'not entailment'},\n",
    "    'mnli': {0: 'entailment', 1: 'neutral', 2: 'contradiction'},\n",
    "    'snli': {-1: 'unknown', 0: 'entailment', 1: 'neutral', 2: 'contradiction'},\n",
    "    'cb': {0: 'entailment', 1: 'contradiction', 2: 'neutral'},\n",
    "    'sst2': {0: 'negative', 1: 'positive'},\n",
    "    'rt': {0: 'negative', 1: 'positive'},\n",
    "    'qqp': {0: 'not duplicate', 1: 'duplicate'},\n",
    "    'mrpc': {0: 'not equivalent', 1: 'equivalent'},\n",
    "    'pawsx': {0: 'not paraphrase', 1: 'paraphrase'},\n",
    "    'copa': {0: 'choice 1', 1: 'choice 2'},\n",
    "    'piqa': {0: 'choice 1', 1: 'choice 2'},\n",
    "    'agn': {0:'world', 1:'sports', 2:'business', 3:'science/technology'},\n",
    "    'trec': {'entities': 'entity', 'description and abstract concepts': 'description and abstract concept',\n",
    "             'human beings': 'human being', 'numeric values': 'numeric value', 'locations': 'location'},\n",
    "    'wsc': {0:'false', 1:'true'},\n",
    "    'teo': {'OFF':'offensive', 'NOT':'not offensive'},\n",
    "    'tei': {0:'not irony', 1:'irony'},\n",
    "    'wic': {0:'false', 1:'true'},\n",
    "    'cola': {0:'unacceptable', 1:'acceptable'},\n",
    "    'wino': {1:'choice 1', 2:'choice 2'},\n",
    "    # evaluation datasets\n",
    "    'teh' : {1:'hate', 0: 'not hate'},\n",
    "    'teab' : {'NONE': 'none', 'FAVOR': 'favor', 'AGAINST': 'against'},\n",
    "    'teat' : {'NONE': 'none', 'FAVOR': 'favor', 'AGAINST': 'against'},\n",
    "    'tefe' : {'NONE': 'none', 'FAVOR': 'favor', 'AGAINST': 'against'},\n",
    "    'tehi' : {'NONE': 'none', 'FAVOR': 'favor', 'AGAINST': 'against'},\n",
    "    'adec' : {1: 'adverse drug event', 2: 'not adverse drug event'},\n",
    "    'or' : {1: 'not overruling', 2: 'overruling'},\n",
    "    'sot' : {1: 'company', 2: 'research institute', 3: 'university'},\n",
    "    'tos' : {1: 'not potentially unfair', 2: 'potentially unfair'},\n",
    "    'tc' : {1: 'complaint', 2: 'no complaint'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datsets = change_labels_in_datasets_with_optional_mappings(mappings, train_without_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#test_datsets = change_labels_in_datasets_with_optional_mappings(mappings, test_without_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def structuring_dataset(dataset, dataset_name, label_col='label'):\n",
    "    \"\"\"\n",
    "    Transforms the dataset by concatenating row values with specific formatting\n",
    "    and adds a dataset name column.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Input dataset.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        label_col (str): Name of the label column. Default is 'label'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed dataset with 'text', 'dataset_name' and 'label' columns.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset, pd.DataFrame):\n",
    "        raise ValueError(\"The input dataset must be a pandas DataFrame.\")\n",
    "    \n",
    "    def concatenate_with_custom_logic(row):\n",
    "        combined_values = []\n",
    "        for col, value in row.items():\n",
    "            if col != label_col:\n",
    "                combined_values.append(f\"{value}\")\n",
    "        return ' \\n '.join(combined_values).strip()\n",
    "\n",
    "    # Create the 'text' column using the custom logic\n",
    "    dataset['text'] = dataset.apply(concatenate_with_custom_logic, axis=1)\n",
    "    dataset['dataset_name'] = dataset_name\n",
    "\n",
    "    # Extract the label column as a separate column\n",
    "    dataset['label'] = dataset[label_col]\n",
    "\n",
    "    # Return only the necessary columns\n",
    "    return dataset[['text', 'dataset_name', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_datsets['wic']\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_dataset = {\n",
    "    name: structuring_dataset(dataset, name)\n",
    "    for name, dataset in train_datsets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "\n",
    "#pre_test_dataset = {\n",
    "#    name: structuring_dataset(dataset, name)\n",
    "#    for name, dataset in test_datsets.items()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.concat(pre_train_dataset.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "# test_dataset = pd.concat(pre_test_dataset.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['dataset_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that each dataset contains no more than 25k examples.\n",
    "\n",
    "def creating_balanced_dataset(dataset):\n",
    "    # Group by 'dataset_name' and process each group\n",
    "    grouped_dataset = dataset.groupby('dataset_name', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=25000, random_state=42) if x.shape[0] > 25000 else x\n",
    "    )\n",
    "    # Reset the index of the resulting dataset\n",
    "    return grouped_dataset.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train_dataset = creating_balanced_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train_dataset['dataset_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(balanced_train_dataset['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#balanced_test_dataset = creating_balanced_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below constructs training or evaluating datasets that will be given to a pre-trained model during training or\n",
    "#  to the symbol-tuned model during evaluation, respectively.\n",
    "\n",
    "# The output for the training dataset will be a PandasDataFrame with the following columns: 'text', 'labels', 'num_exemplars', 'original_labels', and 'remappings'.\n",
    "# Where 'text' corresponds to the inputs of prompts, 'labels' the outputs of the prompts, 'num_exemplars' the number of in-context examples per class,\n",
    "# 'original_labels' is the list of original labels per dataset, and 'remappings' corresponds to the original labels' remappings.\n",
    "# This structure makes it easier to construct full few-shot prompts. These are the elements of the final training dataset.\n",
    "# Note that we use two terms, 'training dataset' and 'final training dataset' in this work.\n",
    "# The reason is that with the function below, we create a training dataset, but it is not the last stage of it, as its elements need to be formatted into the final template,\n",
    "# but the formatting happens inside SFTTrainer because we provide the trainer with the final template and the training dataset as one of the arguments.\n",
    "\n",
    "# For evaluation dataset we follow the same idea: the function below constructs the evaluation dataset with the columns: 'text', 'labels', 'task', 'instruction',\n",
    "# 'relevant label', and 'prompt'. Where 'text' corresponds to the inputs of prompts, 'labels' the outputs of the prompts, 'task' the dataset name, 'instruction'\n",
    "# the boolean variable that specifies whether this element of the evaluation dataset contains instruction or not,\n",
    "#  'relevant label' the boolean variable that specifies whether this element of the evaluation dataset contains relevant labels or not,\n",
    "# 'prompt' contains an instruction prompt.\n",
    "# With this structure, constructing the final evaluation dataset will be more convenient. It will be constructed using the .generate() function from HuggingFace\n",
    "# using the evaluation dataset and the corresponding final template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the function that generates random arbitrary symbols. The source: https://github.com/JerryWeiAI/symbol-tuning\n",
    "# The function was downloaded and added to the folder with this file for convenience.\n",
    "\n",
    "from generate_symbols.generate_symbols import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, random_prompt_number, is_instruction_prompt = False, dataset_name = None):\n",
    "    \"\"\"\n",
    "    Processes a single row, generating a prompt based on the input and label.\n",
    "    \"\"\"\n",
    "    # type(row) is string it means that it's an evaluation example and\n",
    "    # its labels should be in a separate column, not in a template.\n",
    "    \n",
    "    if isinstance(row, str):\n",
    "        input_text, label_text = row, \"\"  # When row is a string, no label (only input text)\n",
    "    else:\n",
    "        input_text, label_text = row['text'], row['label']\n",
    "\n",
    "    # Instructions for creation evaluation dataset    \n",
    "    if is_instruction_prompt and dataset_name:\n",
    "        instructions = {\n",
    "            'subj' : 'Is the following sentence subjective or objective?',\n",
    "            'teh' : 'Label the following tweet based on whether it contains hate speech.',\n",
    "            'teab' : 'Read the following tweet and determine its stance on abortion.',\n",
    "            'teat' : 'Read the following tweet and determine its stance on atheism.',\n",
    "            'tefe' : 'Read the following tweet and determine its stance on feminism.',\n",
    "            'tehi' : 'Read the following tweet and determine its stance on Hillary Clinton.',\n",
    "            'adec' : 'Label the following sentence based on whether it is related to an adverse drug event.',\n",
    "            'or' : 'Label the following sentence based on whether it is overruling or not.',\n",
    "            'sot' : 'Read the following paper title and institution name and classify the institution as a university, company, or research institute.',\n",
    "            'tos' : 'Label the following sentence from a Terms of Service based on whether it is potentially unfair.',\n",
    "            'tc' : 'Label the following tweet text based on whether it contains a complaint.'\n",
    "        }\n",
    "        \n",
    "        instruction = instructions[dataset_name]\n",
    "        prompt = f\"Question: {instruction} \\n {input_text} \\n Answer: {label_text}\"\n",
    "    else:\n",
    "        # Templates\n",
    "        prompts = [\n",
    "            f\"Input: {input_text} \\n Output: {label_text}\",\n",
    "            f\"Input: {input_text} \\n Target: {label_text}\",\n",
    "            f\"Input: {input_text} \\n Symbol: {label_text}\",\n",
    "            f\"Input: {input_text} \\n Label: {label_text}\",\n",
    "            f\"Question: {input_text} \\n Answer: {label_text}\",\n",
    "            f\"Student: {input_text} \\n Teacher: {label_text}\",\n",
    "            f\"X = {input_text} \\n Y = {label_text}\",\n",
    "            f\"Q: {input_text} \\n A: {label_text}\",\n",
    "            f\"{input_text} -> {label_text}\",\n",
    "            f\"Sentences: {input_text} \\n Mapped To: {label_text}\",\n",
    "        ]\n",
    "        \n",
    "        prompt = prompts[random_prompt_number]\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def dataset_with_remapped_labels(dataset, remapp_dic):\n",
    "    \"\"\"Remap labels in the dataset based on the given dictionary.\"\"\"\n",
    "    if not isinstance(remapp_dic, dict):\n",
    "        raise TypeError(f\"Expected remapping variable needs to be a dictionary, got {type(remapp_dic)}\")\n",
    "    dataset['label'] = dataset['label'].map(remapp_dic)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "    \n",
    "\n",
    "def creating_final_datasets(dataset, prompts_number, test_dataset, groupedby,\n",
    "                                         is_train_dataset):\n",
    "    \"\"\"\n",
    "    Creates a training or evaluating dataset by sampling a random number of exemplars per class for the training or 4 \n",
    "    random examples per class for the evaluating dataset,\n",
    "    and applying the `process_row` function to generate text data.\n",
    "    \"\"\"\n",
    "\n",
    "    # if-statements to catch the cases when the datasets\n",
    "    # (either the one for future in-context examples or evaluation examples)\n",
    "    # doesn't have a proper strcture.\n",
    "\n",
    "    if len(dataset.columns) != 3 or len(test_dataset.columns) != 3:\n",
    "        print(\"The dataset for in-context examples or the dataset for evaluation examples doesn't have a proper structure.\")\n",
    "        return\n",
    "\n",
    "    required_columns = ['text', 'dataset_name', 'label']\n",
    "\n",
    "    if not all(col in dataset.columns for col in required_columns) or not all(col in test_dataset.columns for col in required_columns):\n",
    "        print(\"The dataset for in-context examples or the dataset for evaluation examples doesn't have a proper structure.\")\n",
    "        return\n",
    "\n",
    "    final_dataset = []  # List to store the final dataset\n",
    "    \n",
    "    # Creating a dictionary to map the labels from the current dataset to arbitrary symbols\n",
    "    current_labels = dataset['label'].unique().tolist()\n",
    "    num_labels = len(current_labels)\n",
    "    is_eval = not is_train_dataset\n",
    "    random_labels = generate(num_labels, is_eval)\n",
    "    remapping = dict(zip(current_labels, random_labels))\n",
    "\n",
    "    if not is_train_dataset:\n",
    "        # Track number of prompts for each setting:\n",
    "        # with instructions/without instructions, with relevant labels/without relevant labels\n",
    "        promt_num_per_set = prompts_number // 4\n",
    "\n",
    "        # Initial settings\n",
    "        instruct = True\n",
    "        rel_labels = True\n",
    "\n",
    "    # Prepare the test dataset for evaluation examples\n",
    "    if test_dataset.empty:\n",
    "        print(\"The test dataset for evaluation example creation is empty.\")\n",
    "        return\n",
    "\n",
    "    # Creation of final dataset with prompts and remapped labels\n",
    "    for i in range(prompts_number):\n",
    "        \n",
    "        # Set examples per class\n",
    "        if is_train_dataset:\n",
    "            exemplars_per_class = random.randint(2, 10)\n",
    "        else:\n",
    "            exemplars_per_class = 4\n",
    "\n",
    "        # Sample examples from each class\n",
    "        sampled_dataset = (\n",
    "            dataset.groupby('label', group_keys=False)\n",
    "            .apply(lambda x: x.sample(n=min(len(x), exemplars_per_class), random_state=random.randint(0, 9999)))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Switch for constructing the elements of the evaluation dataset within 4 settings\n",
    "        if not is_train_dataset:\n",
    "            if i % promt_num_per_set == 0 and i != prompts_number and i != 0:\n",
    "                if instruct and rel_labels:\n",
    "                    instruct = not instruct\n",
    "                elif not instruct and rel_labels:\n",
    "                    rel_labels = not rel_labels\n",
    "                elif not instruct and not rel_labels:\n",
    "                    instruct = not instruct\n",
    "\n",
    "        # Remap labels if needed\n",
    "        sampled_dataset_new = (\n",
    "            dataset_with_remapped_labels(sampled_dataset, remapping)\n",
    "            if (is_train_dataset or not rel_labels)\n",
    "            else sampled_dataset\n",
    "        )\n",
    "\n",
    "        # Generate a random prompt number\n",
    "        random_prompt_number = random.randint(0, 9)\n",
    "\n",
    "        # Process rows to generate prompts\n",
    "        processed_rows = sampled_dataset_new.apply(\n",
    "            lambda row: process_row(row, random_prompt_number, instruct, groupedby) if not is_train_dataset\n",
    "            else process_row(row, random_prompt_number),\n",
    "            axis=1\n",
    "        ).dropna()\n",
    "\n",
    "\n",
    "        if processed_rows.empty:\n",
    "            print('There is not an in-context example.')\n",
    "\n",
    "        cell = \" \\n \".join(processed_rows).strip()\n",
    "\n",
    "        # Create an evaluation example\n",
    "        sample_test = test_dataset.sample(n=1, random_state=random.randint(0, 9999))  # Ensure one sample\n",
    "        result_text = sample_test.iloc[0]['text']  # Extract only 'text' since 'label' will be in a separate column \n",
    "\n",
    "        # Process the evaluation example\n",
    "        evaluation_ex = process_row(\n",
    "            row=result_text,\n",
    "            random_prompt_number=random_prompt_number,\n",
    "            is_instruction_prompt=instruct if not is_train_dataset else False,\n",
    "            dataset_name=groupedby if not is_train_dataset else None\n",
    "        )\n",
    "\n",
    "        if evaluation_ex is None:\n",
    "            continue\n",
    "\n",
    "        # Combine the cell and evaluation_ex into the final cell\n",
    "        final_cell = cell + ' \\n ' + evaluation_ex\n",
    "\n",
    "        label_value = sample_test['label'].iloc[0]  # Extract label safely\n",
    "\n",
    "        if is_train_dataset or not rel_labels:\n",
    "            label = remapping[label_value]  # Apply remapping\n",
    "        else:\n",
    "            label = label_value  # Use the original label\n",
    "\n",
    "        if not is_train_dataset:\n",
    "            if instruct and rel_labels:\n",
    "                original_labels = str(current_labels)\n",
    "                prompt = f\"This prompt contains relevant labels and instructions. The original natural language labels are {original_labels}.\"\n",
    "            elif not instruct and rel_labels:\n",
    "                original_labels = str(current_labels)\n",
    "                prompt = f\"This prompt contains relevant labels but no instructions. The natural language labels are {original_labels}.\"\n",
    "            elif not instruct and not rel_labels:\n",
    "                original_labels = str(current_labels)\n",
    "                remapped_labels = str(random_labels)\n",
    "                prompt = f\"This prompt contains no relevant labels and no instructions. The original natural language labels {original_labels} have been remapped to {remapped_labels}, respectively.\"\n",
    "            else:\n",
    "                original_labels = str(current_labels)\n",
    "                remapped_labels = str(random_labels)                \n",
    "                prompt = f\"This prompt contains no relevant labels but has instructions. The original natural language labels {original_labels} have been remapped to {remapped_labels}, respectively.\"\n",
    "        \n",
    "        if is_train_dataset:\n",
    "            final_dataset.append({\n",
    "                'text': final_cell,\n",
    "                'labels': label,\n",
    "                'num_exemplars': exemplars_per_class,\n",
    "                'original_labels': current_labels,\n",
    "                'remappings': random_labels\n",
    "                    })\n",
    "        else:\n",
    "            final_dataset.append({\n",
    "                'text': final_cell,\n",
    "                'labels': label,\n",
    "                'task': groupedby,\n",
    "                'instruction': instruct,\n",
    "                'relevant label': rel_labels,\n",
    "                'prompt': prompt\n",
    "                \n",
    "            })\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processed {i} prompts...\")\n",
    "\n",
    "    return pd.DataFrame(final_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_dataset = {}\n",
    "grouped_datasets = balanced_train_dataset.groupby('dataset_name')\n",
    "\n",
    "# This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "# Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "#grouped_datasets = balanced_test_dataset.groupby('dataset_name')\n",
    "\n",
    "for group_key, dataset in grouped_datasets:\n",
    "    \n",
    "\n",
    "    # This rows we used for comparison: whether taken from the different splits in-context and evaluation examples, impacts the performance of the resulting model.\n",
    "    # Since we found out that the model's performance did not differ, there is no need to run the rows.\n",
    "    #corres_test_dataset = balanced_test_dataset.loc[balanced_test_dataset['dataset_name'] == group_key].copy()\n",
    "    \n",
    "    # Call creating_training_dataset function for each group\n",
    "    processed_dataset = creating_final_datasets(dataset=dataset, prompts_number = 25000,\n",
    "                                                test_dataset = dataset,\n",
    "                                                groupedby=group_key, is_train_dataset = True)\n",
    "    \n",
    "    # Ensure that the processed dataset is valid before adding to the dictionary\n",
    "    if not processed_dataset.empty:\n",
    "        final_train_dataset[group_key] = processed_dataset\n",
    "    else:\n",
    "        print(f\"Warning: Empty or invalid dataset returned for {group_key}\")\n",
    "\n",
    "\n",
    "# Concatenate the results into a final DataFrame\n",
    "final_train_df = pd.concat(final_train_dataset.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload pre-train model and corresponding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, AutoTokenizer, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Make sure to get access to the models on their corresponding pages on HuggingFace.\n",
    "# \"meta-llama/Llama-3.1-8B-Instruct\": https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "# \"meta-llama/Llama-3.2-3B-Instruct\": https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "# \"google/gemma-7b-it\": https://huggingface.co/google/gemma-7b-it\n",
    "\n",
    "model_checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#model_checkpoint = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_checkpoint = \"google/gemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "config.hidden_dropout_prob = 0.05  # Dropout rate for hidden layers\n",
    "config.attention_probs_dropout_prob = 0.05  # Dropout rate for attention layers\n",
    "\n",
    "# Load the model with the modified configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.bfloat16,\n",
    "                        attn_implementation=\"flash_attention_2\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fuction that creates elements for the final training dataset\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    text = examples[\"text\"]\n",
    "    label = examples[\"labels\"]\n",
    "    k = str(examples['num_exemplars'])\n",
    "    original_labels = str(examples['original_labels'])\n",
    "    remappings = str(examples['remappings'])\n",
    "\n",
    "    text = f'''### Overview. This prompt contains k = {k} in-context exemplars per class. The original natural language labels {original_labels} have been remapped to {remappings}, respectively.\n",
    "    \n",
    "    ### Prompt:\n",
    "    {text}\n",
    "            \n",
    "    ### Answer:\n",
    "    {label}'''\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTTrainer expects an argument dataset with the appropriate type\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "final_train_ds = Dataset.from_pandas(final_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = final_train_ds.train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for LoRA\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias = 'none',\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    max_steps=2000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=8,  # Reduced gradient accumulation steps\n",
    "    gradient_checkpointing=True,  # Reduce memory usage\n",
    "    dataloader_num_workers=4,  # Use multiple workers for data loading\n",
    "    dataloader_pin_memory=True,  # Speed up data transfer to GPU\n",
    "    packing=True, # Responsible for chunking dataset to the context length and packing it\n",
    "    max_seq_length=512, # Context length\n",
    "    save_safetensors=False,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=2e-05,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val_split['train'],\n",
    "    eval_dataset=train_val_split['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func = formatting_prompts_func, # Creation of full few-shot prompts\n",
    "     \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training and validation losses from the Trainer's log history\n",
    "train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Steps or Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Evaluation datasets\n",
    "\n",
    "eval_datasets = {\n",
    "    'subj' : load_dataset(\"SetFit/subj\", split = 'test'),\n",
    "    'teh' : load_dataset(\"christinacdl/HatEval_2019_Test_Set_Task5\", split = 'train'), \n",
    "    'teab' : load_dataset(\"krishnagarg09/SemEval2016Task6\", split = 'validation'), \n",
    "    'teat' : load_dataset(\"krishnagarg09/SemEval2016Task6\", split = 'validation'),\n",
    "    'tefe' : load_dataset(\"krishnagarg09/SemEval2016Task6\", split = 'validation'),\n",
    "    'tehi' : load_dataset(\"krishnagarg09/SemEval2016Task6\", split = 'validation'),\n",
    "    'adec' : load_dataset(\"ought/raft\", \"ade_corpus_v2\", split = 'train'),\n",
    "    'or' : load_dataset(\"ought/raft\", \"overruling\", split = 'train'),\n",
    "    'sot' : load_dataset(\"ought/raft\", \"semiconductor_org_types\", split = 'train'), \n",
    "    'tos' : load_dataset(\"ought/raft\", \"terms_of_service\", split = 'train'),\n",
    "    'tc' : load_dataset(\"ought/raft\", \"twitter_complaints\", split = 'train') \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the datasets to have one structure: ['text', 'label']. Where 'label' column contains text values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAB, TEAT, TEFE, TEHI datasets are combined in the source. We need to separate them.\n",
    "\n",
    "eval_datasets['teab'] = eval_datasets['teab'].filter(lambda example: example['Target'] == 'Legalization of Abortion')\n",
    "eval_datasets['teat'] = eval_datasets['teat'].filter(lambda example: example['Target'] == 'Atheism')\n",
    "eval_datasets['tefe'] = eval_datasets['tefe'].filter(lambda example: example['Target'] == 'Feminist Movement')\n",
    "eval_datasets['tehi'] = eval_datasets['tehi'].filter(lambda example: example['Target'] == 'Hillary Clinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns from the datasets.\n",
    "\n",
    "without_columns_eval = {\n",
    "    name: remove_columns(name, dataset)\n",
    "    for name, dataset in eval_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBJ dataset has 'label_text' column that represents labels' text values, the 'label' column isn't needed.\n",
    "\n",
    "without_columns_eval['subj'] = without_columns_eval['subj'].drop(columns = 'label')\n",
    "without_columns_eval['subj'] = without_columns_eval['subj'].rename(columns = {'label_text': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEH dataset contains unnecessary column 'text_label'.\n",
    "\n",
    "without_columns_eval['teh'] = without_columns_eval['teh'].drop(columns = 'text_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAB, TEAT, TEFE, TEHI datasets' columns need to be renamed.\n",
    "\n",
    "without_columns_eval['teab'] = without_columns_eval['teab'].rename(columns = {'Tweet':'text', 'Stance': 'label'})\n",
    "without_columns_eval['teat'] = without_columns_eval['teat'].rename(columns = {'Tweet':'text', 'Stance': 'label'})\n",
    "without_columns_eval['tefe'] = without_columns_eval['tefe'].rename(columns = {'Tweet':'text', 'Stance': 'label'})\n",
    "without_columns_eval['tehi'] = without_columns_eval['tehi'].rename(columns = {'Tweet':'text', 'Stance': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADEC, OR, SOT, TOS and TC datasets' columns need to be renamed.\n",
    "\n",
    "change_label_name = ['adec', 'or', 'sot', 'tos', 'tc']\n",
    "\n",
    "for name, dataset in without_columns_eval.items():\n",
    "    if name in change_label_name:\n",
    "        without_columns_eval[name] = without_columns_eval[name].rename(columns = {'Sentence': 'text',\n",
    "                                                                                  'Label': 'label',\n",
    "                                                                                  'Paper title': 'text',\n",
    "                                                                                 'Tweet text': 'text'})\n",
    "                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce number of examples per dataset to 100 random ones.\n",
    "\n",
    "for name, dataset in without_columns_eval.items():\n",
    "    if dataset.shape[0] > 100:\n",
    "        without_columns_eval[name] = dataset.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        without_columns_eval[name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels' values to text labels.\n",
    "\n",
    "eval_datasets = change_labels_in_datasets_with_optional_mappings(mappings, without_columns_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure the datasets according to the necessary structure.\n",
    "\n",
    "eval_datasets = {\n",
    "    name: structuring_dataset(dataset, name)\n",
    "    for name, dataset in eval_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = pd.concat(eval_datasets.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an astute teenager has a major problem that mi...</td>\n",
       "      <td>subj</td>\n",
       "      <td>objective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this riveting world war ii moral suspense stor...</td>\n",
       "      <td>subj</td>\n",
       "      <td>subjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the ring just left me cold and wet like i was ...</td>\n",
       "      <td>subj</td>\n",
       "      <td>subjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a sly female empowerment movie , although not ...</td>\n",
       "      <td>subj</td>\n",
       "      <td>subjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ex-special forces operator frank martin ( jaso...</td>\n",
       "      <td>subj</td>\n",
       "      <td>objective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>@asblough Yep! It should send you a notificati...</td>\n",
       "      <td>tc</td>\n",
       "      <td>no complaint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>@Wavy2Timez for real</td>\n",
       "      <td>tc</td>\n",
       "      <td>no complaint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>@KenyaPower_Care  no power in south b area... ...</td>\n",
       "      <td>tc</td>\n",
       "      <td>complaint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Honda won't do anything about water leaking in...</td>\n",
       "      <td>tc</td>\n",
       "      <td>complaint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>@CBSNews @Dodge @ChryslerCares My driver side ...</td>\n",
       "      <td>tc</td>\n",
       "      <td>complaint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>809 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text dataset_name  \\\n",
       "0    an astute teenager has a major problem that mi...         subj   \n",
       "1    this riveting world war ii moral suspense stor...         subj   \n",
       "2    the ring just left me cold and wet like i was ...         subj   \n",
       "3    a sly female empowerment movie , although not ...         subj   \n",
       "4    ex-special forces operator frank martin ( jaso...         subj   \n",
       "..                                                 ...          ...   \n",
       "804  @asblough Yep! It should send you a notificati...           tc   \n",
       "805                               @Wavy2Timez for real           tc   \n",
       "806  @KenyaPower_Care  no power in south b area... ...           tc   \n",
       "807  Honda won't do anything about water leaking in...           tc   \n",
       "808  @CBSNews @Dodge @ChryslerCares My driver side ...           tc   \n",
       "\n",
       "            label  \n",
       "0       objective  \n",
       "1      subjective  \n",
       "2      subjective  \n",
       "3      subjective  \n",
       "4       objective  \n",
       "..            ...  \n",
       "804  no complaint  \n",
       "805  no complaint  \n",
       "806     complaint  \n",
       "807     complaint  \n",
       "808     complaint  \n",
       "\n",
       "[809 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the evaluation dataset.\n",
    "\n",
    "final_eval_dataset = {}\n",
    "grouped_datasets = eval_dataset.groupby('dataset_name')\n",
    "\n",
    "for group_key, dataset in grouped_datasets:\n",
    "    \n",
    "    # Call creating_final_datasets function for each group\n",
    "    processed_dataset = creating_final_datasets(dataset, 100,\n",
    "                                                dataset, group_key, False)\n",
    "    \n",
    "    # Ensure that the processed dataset is valid before adding to the dictionary\n",
    "    if processed_dataset is not None and not processed_dataset.empty:\n",
    "        final_eval_dataset[group_key] = processed_dataset\n",
    "    else:\n",
    "        print(f\"Warning: Empty or invalid dataset returned for {group_key}\")\n",
    "\n",
    "\n",
    "# Concatenate the results into a final DataFrame\n",
    "final_eval = pd.concat(final_eval_dataset.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>task</th>\n",
       "      <th>instruction</th>\n",
       "      <th>relevant label</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: Label the following sentence based o...</td>\n",
       "      <td>not adverse drug event</td>\n",
       "      <td>adec</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>This prompt contains relevant labels and instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: Label the following sentence based o...</td>\n",
       "      <td>not adverse drug event</td>\n",
       "      <td>adec</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>This prompt contains relevant labels and instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Label the following sentence based o...</td>\n",
       "      <td>adverse drug event</td>\n",
       "      <td>adec</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>This prompt contains relevant labels and instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: Label the following sentence based o...</td>\n",
       "      <td>not adverse drug event</td>\n",
       "      <td>adec</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>This prompt contains relevant labels and instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: Label the following sentence based o...</td>\n",
       "      <td>not adverse drug event</td>\n",
       "      <td>adec</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>This prompt contains relevant labels and instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>Question: Label the following sentence from a ...</td>\n",
       "      <td>64266</td>\n",
       "      <td>tos</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>This prompt contains no relevant labels but ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>Question: Label the following sentence from a ...</td>\n",
       "      <td>64266</td>\n",
       "      <td>tos</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>This prompt contains no relevant labels but ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>Question: Label the following sentence from a ...</td>\n",
       "      <td>thema</td>\n",
       "      <td>tos</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>This prompt contains no relevant labels but ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>Question: Label the following sentence from a ...</td>\n",
       "      <td>64266</td>\n",
       "      <td>tos</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>This prompt contains no relevant labels but ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>Question: Label the following sentence from a ...</td>\n",
       "      <td>64266</td>\n",
       "      <td>tos</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>This prompt contains no relevant labels but ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Question: Label the following sentence based o...   \n",
       "1     Question: Label the following sentence based o...   \n",
       "2     Question: Label the following sentence based o...   \n",
       "3     Question: Label the following sentence based o...   \n",
       "4     Question: Label the following sentence based o...   \n",
       "...                                                 ...   \n",
       "1095  Question: Label the following sentence from a ...   \n",
       "1096  Question: Label the following sentence from a ...   \n",
       "1097  Question: Label the following sentence from a ...   \n",
       "1098  Question: Label the following sentence from a ...   \n",
       "1099  Question: Label the following sentence from a ...   \n",
       "\n",
       "                      labels  task  instruction  relevant label  \\\n",
       "0     not adverse drug event  adec         True            True   \n",
       "1     not adverse drug event  adec         True            True   \n",
       "2         adverse drug event  adec         True            True   \n",
       "3     not adverse drug event  adec         True            True   \n",
       "4     not adverse drug event  adec         True            True   \n",
       "...                      ...   ...          ...             ...   \n",
       "1095                   64266   tos         True           False   \n",
       "1096                   64266   tos         True           False   \n",
       "1097                   thema   tos         True           False   \n",
       "1098                   64266   tos         True           False   \n",
       "1099                   64266   tos         True           False   \n",
       "\n",
       "                                                 prompt  \n",
       "0     This prompt contains relevant labels and instr...  \n",
       "1     This prompt contains relevant labels and instr...  \n",
       "2     This prompt contains relevant labels and instr...  \n",
       "3     This prompt contains relevant labels and instr...  \n",
       "4     This prompt contains relevant labels and instr...  \n",
       "...                                                 ...  \n",
       "1095  This prompt contains no relevant labels but ha...  \n",
       "1096  This prompt contains no relevant labels but ha...  \n",
       "1097  This prompt contains no relevant labels but ha...  \n",
       "1098  This prompt contains no relevant labels but ha...  \n",
       "1099  This prompt contains no relevant labels but ha...  \n",
       "\n",
       "[1100 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for models with adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Paths\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#base_model_name = \"google/gemma-7b-it\"\n",
    "\n",
    "lora_path = \"./results\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "\n",
    "# Load LoRA adapter config\n",
    "peft_config = PeftConfig.from_pretrained(lora_path)\n",
    "print(\"LoRA Config:\", peft_config)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model.to('cuda')\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "# Check if LoRA layers are correctly applied\n",
    "print(model)  # Should list LoRA layers\n",
    "\n",
    "# Merge for faster inference\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for model without adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_path = \"google/gemma-7b-it\"\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that contains the final template.\n",
    "\n",
    "def formatting_func_eval(example):\n",
    "\n",
    "    text = f'''### Overview. {example['prompt']}\n",
    "    ### Prompt:\n",
    "    {example['text']}\n",
    "            \n",
    "    ### Answer:\n",
    "    '''\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "context_length = 512\n",
    "\n",
    "accuracies = defaultdict(list)\n",
    "\n",
    "def calculate_accuracy(dataset, model, tokenizer, task, instruction, relevant_labels):\n",
    "    \n",
    "    required_columns = ['text', 'labels', 'prompt']\n",
    "    predicted_labels = []\n",
    "    correct_predictions = 0\n",
    "    dataset_labels = [str(label).strip().lower() for label in dataset['labels'].dropna().tolist()]\n",
    "\n",
    "    if not all(col in dataset.columns for col in required_columns):\n",
    "        print(\"The dataset doesn't have a proper structure.\")\n",
    "        return\n",
    "    \n",
    "    class_labels = dataset['labels'].unique().tolist()\n",
    "    class_token_ids = [tokenizer.encode(label, add_special_tokens=True) for label in class_labels]\n",
    "    \n",
    "    # Loop through each row in the dataset to generate predictions and compare them with ground truth labels\n",
    "    for idx, row in dataset.iterrows():\n",
    "\n",
    "\n",
    "        # Tokenize the input text\n",
    "        input_encodings = tokenizer(\n",
    "            formatting_func_eval(row),\n",
    "            truncation=False, \n",
    "            max_length=context_length,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Generate output conditioned by the provided inputs, i.e., input_encodings\n",
    "        generated_ids = model.generate(\n",
    "            input_encodings['input_ids'].to('cuda'),\n",
    "            attention_mask=input_encodings['attention_mask'].to('cuda'),\n",
    "            max_new_tokens=10, \n",
    "            do_sample=False,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            force_words_ids=class_token_ids,  # Forces output to be one of the class labels\n",
    "            num_beams=2, # beam-search decoding, because num_beams > 1 and do_sample=False\n",
    "        )\n",
    "\n",
    "        # To see only newly generated tokens without provided input\n",
    "        new_tokens = generated_ids[:, len(input_encodings['input_ids'][0]):]\n",
    "\n",
    "        # Decode the generated token IDs into text\n",
    "        generated_text = tokenizer.decode(new_tokens[0], skip_special_tokens=True).strip().lower()\n",
    "        \n",
    "        # Initialize final_label as 'other' by default\n",
    "        final_label = 'other'\n",
    "\n",
    "        # Check if generated_text starts with any of the truth labels\n",
    "        for truth_label in dataset_labels:\n",
    "            if generated_text.startswith(truth_label):\n",
    "                final_label = truth_label\n",
    "                break\n",
    "            \n",
    "        if row['labels'].strip().lower() == final_label.strip().lower():\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        predicted_labels.append(final_label.strip().lower())\n",
    "        \n",
    "    actual_labels = dataset_labels\n",
    "    labels = list(set(dataset_labels))\n",
    "    \n",
    "    if 'other' in predicted_labels:\n",
    "        labels.append('other')\n",
    "\n",
    "    \n",
    "    confusion_matrix = metrics.confusion_matrix(actual_labels, predicted_labels, labels = labels)\n",
    "    \n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels = labels)\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Confusion Matrix for {task} with settings:\\n Instructions={instruction} and original labels={relevant_labels}\")\n",
    "    plt.show()\n",
    "    \n",
    "    def safe_div(numerator, denominator):\n",
    "        return numerator / denominator if denominator > 0 else None\n",
    "    \n",
    "    setting_accuracy = safe_div(correct_predictions,dataset.shape[0])\n",
    "    \n",
    "    print(f\"\"\"Accuracy for {task} with settings:\\n Instructions={instruction} \n",
    "          and original labels={relevant_labels} is {setting_accuracy:.4f}\"\"\")\n",
    "\n",
    "    \n",
    "    accuracies[task].append(setting_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the symbol-tuned model in 4 settings\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Define the custom sorting order for 'instruction' and 'relevant label'\n",
    "instruction_order = [False, True]  # False comes first\n",
    "relevant_label_order = [False, True]  # False comes first\n",
    "\n",
    "# Convert columns to categorical types with the specified order\n",
    "final_eval['instruction'] = final_eval['instruction'].astype(CategoricalDtype(instruction_order, ordered=True))\n",
    "final_eval['relevant label'] = final_eval['relevant label'].astype(CategoricalDtype(relevant_label_order, ordered=True))\n",
    "\n",
    "# Sort the DataFrame according to the custom order\n",
    "final_eval = final_eval.sort_values(by=['task', 'instruction', 'relevant label'])\n",
    "\n",
    "group_columns = ['task', 'instruction', 'relevant label'] \n",
    "grouped_datasets = final_eval.groupby(group_columns)\n",
    "\n",
    "for group_keys, group_dataset in grouped_datasets:\n",
    "    struct_dataset = group_dataset.drop(columns = group_columns).reset_index(drop=True)\n",
    "    calculate_accuracy(struct_dataset, model, tokenizer, group_keys[0], group_keys[1], group_keys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the calculated accuracies\n",
    "\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average accuracy among settings and tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sums to 0\n",
    "no_inst_irrel_lb = 0\n",
    "no_inst_rel_lb = 0\n",
    "instr_irrel_lb = 0\n",
    "inst_rel_lb = 0\n",
    "\n",
    "# Loop through the dictionary\n",
    "for key, value in accuracies.items():  \n",
    "    if len(value) > 0:\n",
    "        no_inst_irrel_lb += value[0]\n",
    "    if len(value) > 1:\n",
    "        no_inst_rel_lb += value[1]\n",
    "    if len(value) > 2:\n",
    "        instr_irrel_lb += value[2]\n",
    "    if len(value) > 3:\n",
    "        inst_rel_lb += value[3]\n",
    "        \n",
    "acc_no_inst_irrel_lb = no_inst_irrel_lb / len(accuracies)\n",
    "acc_no_inst_rel_lb = no_inst_rel_lb / len(accuracies)\n",
    "acc_instr_irrel_lb = instr_irrel_lb / len(accuracies)\n",
    "acc_inst_rel_lb = inst_rel_lb / len(accuracies)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "      Average Model Accuracy for settings: without instructions and with remapped labels is {acc_no_inst_irrel_lb:.4f}\\n \n",
    "      Average Model Accuracy for settings: without instructions and with original labels is {acc_no_inst_rel_lb:.4f}\\n\n",
    "      Average Model Accuracy for settings: with instructions and with remapped labels is {acc_instr_irrel_lb:.4f}\\n\n",
    "      Average Model Accuracy for settings: with instructions and with original labels is {acc_inst_rel_lb:.4f}\n",
    "      \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additinal blocks to evaluate models on 5-shot MMLU if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fun_mmlu(example):\n",
    "    \n",
    "    text = f'''### Overview. This prompt contains relevant labels and instructions. The original natural language labels\n",
    "are [“A”, “B”, “C”, “D”].\n",
    "    ### Prompt:\n",
    "    {example['text']}\n",
    "    '''\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "mmlu_5shot = load_dataset(\"FuryMartin/Ianvs-MMLU-5-shot\")\n",
    "mmlu_5shot = mmlu_5shot['test']\n",
    "mmlu_5shot_fin = mmlu_5shot.remove_columns([\"prompt\", 'explanation', 'level_1_dim', 'level_2_dim', 'level_3_dim', 'level_4_dim'])\n",
    "mmlu = mmlu_5shot_fin.to_pandas()\n",
    "mmlu = mmlu.rename(columns = {'query': 'text', 'response': 'labels'})\n",
    "\n",
    "mmlu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(mmlu, model, tokenizer, 'MMLU 5-shot', True, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
